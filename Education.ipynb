{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,  TensorDataset, Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "#import os\n",
    "import shutil\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from torchvision import models, datasets, transforms\n",
    "\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.data.sort()\n",
    "        self.targets.sort()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.targets[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data = np.load('train_val_cropped dataset_0804_wb.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data['arr_0'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_images = torch.from_numpy(data['arr_0']/255)\n",
    "print(x_images.shape)\n",
    "x_images = torch.transpose(x_images, 1, 3)\n",
    "print(x_images.shape)\n",
    "y_labels = torch.from_numpy(data['arr_1'])\n",
    "print(y_labels.shape)\n",
    "y_labels = torch.transpose(y_labels, 1, 3) \n",
    "print(y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_dataset = len(y_labels) \n",
    "\n",
    "dataset = MyDataset(data=x_images.float(), targets=y_labels)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(len_dataset*0.8), len_dataset-int(len_dataset*0.8)])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 6, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 6, shuffle = False)\n",
    "\n",
    "del(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(model, valloader, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            #print(labels.shape, 'labels')\n",
    "            outputs = model(images)\n",
    "            #print(outputs.shape, 'outputs_val')\n",
    "            #_, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = outputs\n",
    "            \n",
    "            #labels=labels.squeeze(1)\n",
    "            #labels=labels.reshape(labels, )\n",
    "            #print(labels.shape, 'labels')\n",
    "            #print(predicted.shape, 'predicted')\n",
    "            \n",
    "            #predicted[0].show()\n",
    "            total += labels.size(0)\n",
    "            a = (predicted == labels).sum().item()\n",
    "            correct += a\n",
    "            \n",
    "            # print(correct / total)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, num_epochs=50):\n",
    "    \n",
    "    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #print('Using device:', device)\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    sum_acc = np.zeros((1,  num_epochs))\n",
    "    sum_loss = sum_acc.copy()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = smp.losses.DiceLoss('multiclass')\n",
    "    #\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    best_accuracy = 0\n",
    "    best_loss = 100\n",
    "    #fig = plt.figure() \n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        for img_batch, labels_batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            #print(img_batch.shape)\n",
    "            #print(labels_batch.shape)\n",
    "        \n",
    "            output = model(img_batch.to(device))\n",
    "            \n",
    "            #print(output.shape, 'output')\n",
    "    \n",
    "            #loss = criterion(output, labels_batch.to(device).squeeze().long())\n",
    "            loss = criterion(output, torch.argmax(labels_batch.to(device).squeeze().long(),dim = 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            images = img_batch.cpu()\n",
    "            label_nums = output.cpu() \n",
    "            accuracy = validate(model, val_loader, device)\n",
    "#             print(accuracy)\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            print('Best accuracy improved')\n",
    "            torch.save(model.state_dict(), 'model_weights.pth') #name of saved weights\n",
    "        if best_loss > loss.cpu().item():\n",
    "            best_loss = loss.cpu().item()\n",
    "#             print('Best loss improved')\n",
    "\n",
    "        sum_acc[0, epoch] = accuracy\n",
    "        sum_loss[0, epoch] = loss  \n",
    "        epoch_end = time.time()\n",
    "        print(\"Epoch: {} Loss: {:.3f} Accuracy: {:.3f} Time: {:.4f}s\".format(epoch, loss.item(), accuracy, epoch_end-epoch_start))\n",
    "        \n",
    "    \n",
    "    return sum_acc, sum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(encoder_name='resnet34', \n",
    "                 encoder_depth=5, \n",
    "                 encoder_weights='imagenet', \n",
    "                 decoder_use_batchnorm=True, \n",
    "                 decoder_channels=(256, 128, 64, 32, 16), \n",
    "                 decoder_attention_type=None, \n",
    "                 in_channels=3, \n",
    "                 classes=3, \n",
    "                 activation='softmax2d', \n",
    "                 aux_params=None)\n",
    "\n",
    "accuracy, loss = train(model, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
